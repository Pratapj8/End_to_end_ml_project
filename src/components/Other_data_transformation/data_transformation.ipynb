{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08619f03",
   "metadata": {},
   "source": [
    "Nice. You want this neat little transformer to play with MySQL, MongoDB, and AWS, behave in production, and not leak credentials like a toddler with a bucket of sand. Fine. Below is a practical, slightly snarky, but fully actionable plan: code snippets, patterns, and a production-readiness checklist. Iâ€™ll also show secure ways to store credentials (no hardcoding, no sticky notes). ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "# 1) How to plug your `DataTransformation` into data sources (MySQL, MongoDB, S3)\n",
    "\n",
    "Use lightweight adapters that return `pandas.DataFrame`. Keep ingestion separate from transformation â€” single responsibility. Example wrappers:\n",
    "\n",
    "```python\n",
    "# connectors.py\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from pymongo import MongoClient\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "def read_mysql(query: str, secret: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    secret: {\"user\":..., \"password\":..., \"host\":..., \"port\":..., \"database\":...}\n",
    "    \"\"\"\n",
    "    uri = f\"mysql+pymysql://{secret['user']}:{secret['password']}@{secret['host']}:{secret.get('port',3306)}/{secret['database']}\"\n",
    "    engine = create_engine(uri)\n",
    "    with engine.connect() as conn:\n",
    "        return pd.read_sql(query, conn)\n",
    "\n",
    "\n",
    "def read_mongodb(collection: str, query: dict, secret: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    secret: {\"uri\": \"mongodb+srv://...\"} or {\"host\":..., \"port\":..., \"username\":..., \"password\":...}\n",
    "    \"\"\"\n",
    "    client = MongoClient(secret[\"uri\"])\n",
    "    db = client.get_database(secret.get(\"database\"))\n",
    "    docs = list(db[collection].find(query))\n",
    "    return pd.DataFrame(docs)\n",
    "\n",
    "\n",
    "def read_csv_from_s3(bucket: str, key: str, aws_credentials: dict=None) -> pd.DataFrame:\n",
    "    s3 = boto3.client(\"s3\", **(aws_credentials or {}))\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(obj['Body'])\n",
    "```\n",
    "\n",
    "Then call your transformer:\n",
    "\n",
    "```python\n",
    "from connectors import read_mysql, read_mongodb, read_csv_from_s3\n",
    "from data_transformation import DataTransformation  # your class\n",
    "\n",
    "dt = DataTransformation()\n",
    "\n",
    "# Example: fetch train/test from MySQL\n",
    "train_df = read_mysql(\"SELECT * FROM train_table\", secret=mysql_secret)\n",
    "test_df = read_mysql(\"SELECT * FROM test_table\", secret=mysql_secret)\n",
    "\n",
    "# Or from MongoDB\n",
    "# train_df = read_mongodb(\"students\", {\"split\":\"train\"}, secret=mongo_secret)\n",
    "\n",
    "# Apply your pipeline on DataFrames (slight wrapper)\n",
    "preprocessor = dt.get_data_transformer_object()\n",
    "X_train = train_df.drop(columns=[\"math_score\"])\n",
    "y_train = train_df[\"math_score\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"math_score\"])\n",
    "y_test = test_df[\"math_score\"]\n",
    "\n",
    "X_train_tr = preprocessor.fit_transform(X_train)\n",
    "X_test_tr = preprocessor.transform(X_test)\n",
    "```\n",
    "\n",
    "If you want the `initiate_data_transformation` signature preserved (paths), you can always create temporary CSVs in memory or on disk from the DB DataFrames and pass their paths â€” but better: add an overload to accept DataFrames directly.\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Save artifacts to AWS (S3) â€” recommended for production\n",
    "\n",
    "Save the preprocessor to S3 so all workers can access it.\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import pickle\n",
    "\n",
    "def upload_object_to_s3(obj, bucket, key, aws_credentials=None):\n",
    "    data = pickle.dumps(obj)\n",
    "    s3 = boto3.client(\"s3\", **(aws_credentials or {}))\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=data)\n",
    "```\n",
    "\n",
    "Call this after fitting your preprocessor instead of saving only to local `artifacts/`.\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Secure credential handling (no hardcoding, please)\n",
    "\n",
    "Pick one depending on your environment. Never keep secrets in code.\n",
    "\n",
    "* **AWS (recommended on AWS infra)**\n",
    "\n",
    "  * Use **IAM Roles** for EC2/ECS/Lambda/Batch â€” avoids long-lived keys.\n",
    "  * Store secrets in **AWS Secrets Manager** or **SSM Parameter Store** (encrypted with KMS).\n",
    "  * Example: retrieve DB creds from Secrets Manager at runtime with `boto3.client('secretsmanager').get_secret_value()`.\n",
    "\n",
    "* **Kubernetes / EKS**\n",
    "\n",
    "  * Use **Kubernetes Secrets** (backed by AWS KMS) or integrate HashiCorp Vault (recommended for larger orgs).\n",
    "  * Mount secrets as env vars or files in pods; prefer files to avoid logging leaks.\n",
    "\n",
    "* **Local/dev**\n",
    "\n",
    "  * Use a `.env` file with `python-dotenv` (never commit that file).\n",
    "  * For CI, inject secrets through the pipelineâ€™s secret store (GitHub Actions Secrets, GitLab CI/CD variables).\n",
    "\n",
    "* **HashiCorp Vault** â€” if you want central control, leasing, dynamic DB credentials, and rotation.\n",
    "\n",
    "Short snippet: fetching from AWS Secrets Manager\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "def get_secret(secret_name, region_name=\"us-east-1\"):\n",
    "    client = boto3.client(\"secretsmanager\", region_name=region_name)\n",
    "    resp = client.get_secret_value(SecretId=secret_name)\n",
    "    secret = resp.get(\"SecretString\")\n",
    "    return json.loads(secret)\n",
    "```\n",
    "\n",
    "Use IAM role attached to the compute node â€” no long-lived credentials in containers.\n",
    "\n",
    "---\n",
    "\n",
    "# 4) Production-readiness checklist (concrete + actionable)\n",
    "\n",
    "You want a list? Fine. Implement these.\n",
    "\n",
    "**Code / infra**\n",
    "\n",
    "* Containerize: Build reproducible Docker images (pin Python versions and libs).\n",
    "* Infrastructure as code: Terraform / CloudFormation for infra reproducibility.\n",
    "* CI/CD: Automated pipelines to build, test, and deploy containers and artifacts.\n",
    "\n",
    "**Data & pipeline robustness**\n",
    "\n",
    "* Schema validation: use Great Expectations or pandera to validate incoming schemas and fail fast on drift.\n",
    "* Idempotency: ensure jobs can re-run without corrupting state.\n",
    "* Retries & backoff: robust DB/S3 calls with retries (exponential backoff).\n",
    "* Batching vs streaming: design for the intended load (cron/airflow for batch, Kafka/Kinesis for streaming).\n",
    "\n",
    "**Testing**\n",
    "\n",
    "* Unit tests for transformers (edge cases, missing values, unseen categories).\n",
    "* Integration tests that run on ephemeral infra (test DB, test S3).\n",
    "* Data contract tests (CI gates).\n",
    "\n",
    "**Observability**\n",
    "\n",
    "* Structured logging (JSON), correlation ids.\n",
    "* Metrics (Prometheus) for job durations, error rates, rows processed.\n",
    "* Tracing (OpenTelemetry) if you care about root cause.\n",
    "* Alerts on high error rates or schema drift.\n",
    "\n",
    "**Security & compliance**\n",
    "\n",
    "* Secrets stored in Secrets Manager / Vault.\n",
    "* Least-privilege IAM policies.\n",
    "* Encrypt artifacts at rest (S3 + KMS).\n",
    "* Audit logging (CloudTrail).\n",
    "\n",
    "**Deployment**\n",
    "\n",
    "* Deploy preprocessor as part of model CI: build artifact, store versioned preprocessor in S3, record metadata (git SHA, package versions) in a model registry or DB.\n",
    "* Use blue/green or canary for risky changes.\n",
    "\n",
    "**Performance**\n",
    "\n",
    "* Test memory/use with representative data.\n",
    "* Consider lazy transforms or chunked transforms if dataset is large.\n",
    "* Scale horizontally with parallel workers if needed.\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Example: full pipeline flow (production-ready pattern)\n",
    "\n",
    "1. **Ingestion job** pulls data from MySQL/MongoDB/S3 into DataFrame.\n",
    "2. **Schema check** (Great Expectations/pandera). If fail -> alert + stop.\n",
    "3. **Transform** using your `DataTransformation.get_data_transformer_object()` (fit only in training pipeline; transform in inference).\n",
    "4. **Persist preprocessor** to versioned S3 key: `s3://my-bucket/preprocessors/preprocessor_{git_sha}_{timestamp}.pkl`. Store metadata in DynamoDB or RDS.\n",
    "5. **Train** model with transformed arrays. Save model artifacts similarly.\n",
    "6. **Serving** loads model + preprocessor from S3 and applies transform to incoming requests. (Prefer serverless or model-serving infra with IAM roles).\n",
    "7. Logging + metrics + alerting throughout.\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Minimal code sample: secure MySQL fetch + transform + save to S3\n",
    "\n",
    "```python\n",
    "# assume get_secret(\"prod/mysql\") returns a dict with connection info\n",
    "mysql_secret = get_secret(\"prod/mysql\", region_name=\"ap-south-1\")\n",
    "\n",
    "train_df = read_mysql(\"SELECT * FROM train_table\", secret=mysql_secret)\n",
    "test_df  = read_mysql(\"SELECT * FROM test_table\", secret=mysql_secret)\n",
    "\n",
    "dt = DataTransformation()\n",
    "preprocessor = dt.get_data_transformer_object()\n",
    "\n",
    "X_train = train_df.drop(columns=[\"math_score\"])\n",
    "y_train = train_df[\"math_score\"]\n",
    "\n",
    "X_train_tr = preprocessor.fit_transform(X_train)\n",
    "X_test_tr = preprocessor.transform(test_df.drop(columns=[\"math_score\"]))\n",
    "\n",
    "# Save to s3\n",
    "upload_object_to_s3(preprocessor, \"my-artifacts-bucket\", \"preprocessors/preprocessor_v1.pkl\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Secure dev vs prod patterns (short)\n",
    "\n",
    "* **Dev**: `.env`, local credentials, mocked services, limited privileges.\n",
    "* **Prod**: IAM roles + Secrets Manager, KMS, encrypted S3, VPC-restricted databases, private subnets, and no secret values in env or code.\n",
    "\n",
    "---\n",
    "\n",
    "# 8) Extra suggestions for real production maturity\n",
    "\n",
    "* Version your preprocessor and store metadata (Sklearn version, pipeline code hash).\n",
    "* Use a model registry (MLflow, Sagemaker, or custom DB).\n",
    "* Automate secret rotation and use short-lived credentials if possible.\n",
    "* Periodic data drift checks and re-training pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "# TL;DR â€” The minimal things you must do now\n",
    "\n",
    "1. Stop hardcoding secrets. Use Secrets Manager / Vault / env var injection for CI. âœ…\n",
    "2. Containerize your app and attach an IAM role to the compute instance. âœ…\n",
    "3. Save preprocessor artifacts in S3 with versioning (and keep metadata in a registry). âœ…\n",
    "4. Add schema checks + unit tests for the transformer. âœ…\n",
    "5. Add logging/metrics and CI/CD for reproducible deploys. âœ…\n",
    "\n",
    "---\n",
    "\n",
    "There. Thatâ€™s a production-grade roadmap you can follow without pleading to the cloud gods. Want code templates for Terraform, a CI pipeline (GitHub Actions), or a small Dockerfile to start with? I can draft them â€” begrudgingly helpful as always.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
